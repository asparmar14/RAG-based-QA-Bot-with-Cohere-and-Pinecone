# RAG-based QA Bot with Cohere and Pinecone

### Special Thanks to 'Sample Set' for assigning this exciting challenge and helping me grow my skills!

This repository contains an implementation of a **Retrieval-Augmented Generation (RAG) model** for a Question Answering (QA) bot. The bot retrieves relevant information from documents stored in a vector database (Pinecone) and generates coherent answers using Cohere's language model.

## Project Overview

The project is divided into two parts:

1. **Part 1: RAG Model for QA Bot**
   - A pipeline that loads documents, embeds them using `SentenceTransformers`, stores them in **Pinecone**, and retrieves relevant chunks of the document based on user queries.
   - The relevant document chunks are then passed to **Cohere's language model**, which generates a response.

2. **Part 2: Interactive QA Bot Interface**
   - A simple frontend interface built using **Streamlit** (or **Gradio**), allowing users to upload documents and ask questions in real-time.
   - The backend processes the uploaded documents, stores them in Pinecone, and provides real-time answers to user queries.

## Features

- **Vector Database**: Uses **Pinecone** to store document embeddings and retrieve relevant sections.
- **Text Embeddings**: Utilizes `SentenceTransformers` for embedding documents and user queries into vector space.
- **Text Generation**: Uses **Cohere** for generating coherent answers based on retrieved context.
- **Interactive Interface**: Allows users to upload documents and ask questions in real-time.
- **Deployment**: Dockerized for easy deployment.

## Technology Stack

- **Pinecone**: A vector database to store and retrieve document embeddings.
- **Cohere**: A generative language model for producing natural language answers.
- **SentenceTransformers**: For embedding document and query text.
- **Streamlit** (or **Gradio**): Frontend framework for the interactive QA bot interface.
- **Docker**: Containerization for deployment.

## Installation

### Prerequisites
- Python 3.8+
- API keys for Pinecone and Cohere

### Step-by-Step Instructions

1. **Clone the repository:**

   ```bash
   git clone https://github.com/yourusername/qa-bot-rag-cohere.git
   cd qa-bot-rag-cohere

2. Create and activate a virtual environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`

3. Install dependencies:

   ```bash
   pip install -r requirements.txt

4. Set your environment variables:
   Create a .env file in the root directory and add your API keys:

   ```bash
   COHERE_API_KEY=your-cohere-api-key
   PINECONE_API_KEY=your-pinecone-api-key

5. Run the application:
   ```bash
   streamlit run app.py  # or python app.py for a non-streamlit interface

## Usage

### Part 1: RAG Pipeline
The RAG (Retrieval-Augmented Generation) pipeline consists of the following steps:
1. **Document Upload**: The user uploads a document, which is then split into smaller chunks and embedded using `SentenceTransformers`. These embeddings are stored in the Pinecone vector database for retrieval.
2. **Query Handling**: When a user submits a question, the bot embeds the question, searches for the most relevant chunks in Pinecone, and retrieves them.
3. **Answer Generation**: The relevant chunks are passed to Cohere's language model, which generates a coherent answer.

To use the RAG pipeline, follow these steps:

- **Step 1**: Upload the document into the system (in the case of command-line usage, you can replace the document text directly in the code).
- **Step 2**: Ask a question based on the content of the document.
- **Step 3**: The bot retrieves relevant segments from the document and generates an answer using Cohere.

### Part 2: Interactive QA Bot Interface
The interactive interface allows users to:
1. **Upload Documents**: The user can upload a PDF or text file through a simple interface.
2. **Ask Questions**: After the document is uploaded, users can ask questions related to the document's content in real time.
3. **Receive Answers**: The bot displays both the retrieved document segments and the generated answer from Cohere.

To use the interface:
1. **Start the app**:
   - Run the application with `streamlit run app.py` (for a Streamlit interface) or `python app.py` (if using a simpler frontend interface).
2. **Upload a document**: Click the "Upload" button in the web interface to upload your document.
3. **Ask a question**: Type your question in the input field and press enter.
4. **View the result**: The system will display relevant sections of the document and a generated answer.

Here’s an example query you can try:

- **Question:** "What is the purpose of this document?"
- **Answer:** Generated by the Cohere model based on the context retrieved from the document.

### Example Walkthrough

1. **Document Upload**:
   - You can upload any text or PDF file through the interface. The document is split into manageable chunks, embedded using the `SentenceTransformers` model (`all-MiniLM-L6-v2`), and stored in the Pinecone vector database.

2. **Ask a Question**:
   - After uploading the document, input a query based on the content of the document. The system will embed your query and search for relevant sections of the document from the Pinecone database.

3. **Generate Answer**:
   - Once the relevant document chunks are retrieved, they are concatenated and passed to Cohere’s language model, which generates a coherent, human-like answer. The answer is then displayed along with the related document chunks.

### Example:

- **Document**: Let's say you've uploaded a document that describes the technical features of a new software.
  
- **Question**: "What is the main feature of the software?"
  
- **Answer**: The QA bot will retrieve the relevant text describing the main features of the software and generate a response using the Cohere model, such as:
  
  *"The main feature of the software is its ability to integrate seamlessly with cloud platforms, offering enhanced scalability and real-time data processing."*

### Running with Docker

To make it easier to run and deploy, the application is fully containerized. You can run the entire system using Docker.

1. **Build the Docker image**:

   ```bash
   docker build -t qa-bot-rag-cohere .

2. Run the Docker container:
   ```bash
   docker run -p 8501:8501 qa-bot-rag-cohere

3. Access the application:
 - Once the Docker container is running, you can access the interactive QA bot interface via your browser at 'http://localhost:8501'.

### Example Queries:
Here are a few more example queries you can try after uploading a document:

- Question: "How does the software handle scalability?"
- Answer: Generated by Cohere, summarizing the scalability features from the relevant document sections.

- Question: "What are the limitations of the tool?"
- Answer: The bot retrieves sections that mention limitations and generates a clear response.

### Configuration Notes
- Pinecone Index Setup: The Pinecone index uses 384 dimensions to match the output from the 'all-MiniLM-L6-v2' embedding model. If you use a different model with different output dimensions, ensure the Pinecone index is configured to match the new model's output size.

- Cohere Model Setup: The Cohere model used for generating answers is 'command-xlarge-nightly'. You can configure the system to use other models by modifying the 'cohere.generate' function in the code. Adjust the 'max_tokens' and 'temperature' parameters to fine-tune the generation output.

- Frontend Setup: The default setup uses Streamlit for the interactive frontend. If you want to switch to a different frontend framework like Gradio, modify the 'app.py' file accordingly to fit your needs.

### Directory Structure

    ```bash
    ├── __pycache__                
    ├── backend.py/                       # All backend code here
    ├── DockerFile/                       # Docker-specific configuration
    ├── gradio_app.py/                    # Gradio specific files
    ├── notebooks/                        # Jupyter notebooks for experimentation
        └── part-1_colab-notebook.ipynb
    ├── requirements.txt/                 # Store requirements related files
    ├── streamlit_app.py/                 # Streamlit specific files
    └── assets/                           # Any images or static assets
        └── Streamlit-app.jpg

## Acknowledgments

- **Pinecone**: For providing an efficient vector database for fast and scalable search.
- **Cohere**: For providing state-of-the-art language models for generating responses.
- **SentenceTransformers**: For easy-to-use sentence embedding models.

### Special Thanks

A big thank you to **Sample Set** for providing this assignment and helping me grow my skills. I learned a lot while working on this project!

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
