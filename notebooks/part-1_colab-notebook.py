# -*- coding: utf-8 -*-
"""Assignment-solution_Sample-Set.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1biQ6fU_MWBw4V-YNJP3wm8MeAanP5w9W
"""

!pip install pinecone-client sentence-transformers

!pip install cohere

import os
from pinecone import Pinecone, ServerlessSpec
import openai
from sentence_transformers import SentenceTransformer

# Pinecone API setup
PINECONE_API_KEY = 'your-api-key'
PINECONE_ENV = 'us-east-1'  # Ensure you're using a valid region

# Create Pinecone instance
pc = Pinecone(api_key=PINECONE_API_KEY)

# Model for document embeddings (384 dimensions)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize Pinecone index with 384 dimensions
index_name = 'qa-bot-index'

# Check if the index exists, otherwise create it
if index_name in pc.list_indexes().names():
    pc.delete_index(index_name) # Delete the index if it exists

pc.create_index(
    name=index_name,
    dimension=384,  # Match the model's output dimension
    metric="cosine",  # Use cosine similarity for matching
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)

# Access the index
index = pc.Index(index_name)

# Function to embed text
def embed_text(text):
    return model.encode([text], convert_to_tensor=False).tolist()[0]

# Function to add document to Pinecone
def add_document_to_pinecone(document_id, document_text):
    # Split document into smaller chunks (you can adjust the chunk size)
    chunk_size = 300
    chunks = [document_text[i:i+chunk_size] for i in range(0, len(document_text), chunk_size)]

    for i, chunk in enumerate(chunks):
        embedding = embed_text(chunk)
        # Add embedding and metadata to Pinecone
        index.upsert([(f'{document_id}-{i}', embedding, {'text': chunk})])

# Example document (you can replace this with actual PDF extraction)
document_text = """Your document text goes here..."""
document_id = "document1"
add_document_to_pinecone(document_id, document_text)

def retrieve_relevant_chunks(question, top_k=3):
    query_embedding = embed_text(question)
    # Retrieve the most similar chunks from Pinecone
    # Use keyword arguments for vector and top_k
    result = index.query(vector=[query_embedding], top_k=top_k, include_metadata=True)
    return result['matches']

# Example query
question = "What is the summary of this document?"
relevant_chunks = retrieve_relevant_chunks(question)
for match in relevant_chunks:
    print(f"Score: {match['score']}, Text: {match['metadata']['text']}")

import cohere

# Initialize Cohere API client
COHERE_API_KEY = 'your-api-key'  
co = cohere.Client(COHERE_API_KEY)

def generate_answer(question, relevant_chunks):
    # Combine the relevant chunks into context
    context = " ".join([chunk['metadata']['text'] for chunk in relevant_chunks])
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"

    # Call Cohere API to generate the answer
    response = co.generate(
        model='command-xlarge-nightly',  # Use a specific model, you can change this to any Cohere model
        prompt=prompt,
        max_tokens=150,
        temperature=0.7  # Adjust the creativity level if needed
    )

    # Return the generated answer
    return response.generations[0].text.strip()

# Example usage
question = "What is the purpose of the document?"
relevant_chunks = [{'metadata': {'text': 'This document describes the features of a QA system using embeddings...'}}]
answer = generate_answer(question, relevant_chunks)
print("Answer:", answer)

def qa_pipeline(document_text, question):
    # Step 1: Add document to Pinecone
    document_id = "doc1"
    add_document_to_pinecone(document_id, document_text)

    # Step 2: Retrieve relevant document chunks for the question
    relevant_chunks = retrieve_relevant_chunks(question)

    # Step 3: Generate an answer based on the retrieved chunks
    answer = generate_answer(question, relevant_chunks)
    return answer

# Example document and query
document_text = """This is the text of the document you are querying from..."""
question = "What is the main purpose of the document?"

answer = qa_pipeline(document_text, question)
print("Final Answer:", answer)



"""# testing"""

import unittest

# Dummy implementations for the example, replace with actual implementations
def embed_text(text):
    # Example: return a dummy embedding with 384 dimensions
    return [0] * 384

def qa_pipeline(document_text, question):
    # Example: return a dummy answer
    return "Dummy answer"

class TestQAPipeline(unittest.TestCase):
    def test_embed_text(self):
        text = "This is a test sentence."
        embedding = embed_text(text)
        self.assertEqual(len(embedding), 384)  # Check embedding dimension

    def test_qa_pipeline(self):
        document_text = "Example document content."
        question = "What is this document about?"
        answer = qa_pipeline(document_text, question)
        self.assertIsNotNone(answer)  # Check if an answer is generated

# Run tests
def run_tests():
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestQAPipeline)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    return result

# Execute the test function
run_tests()

